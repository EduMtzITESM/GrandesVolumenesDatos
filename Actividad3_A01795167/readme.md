# Grandes Vol煤menes de datos

+ Estudiante: Eduardo Selim Mart铆nez Mayorga

# Actividad 3: Aprendizaje supervisado y no supervisado

# Parte 1: Introducci贸n te贸rica

## Machine Learning

En machine learning se suele caracterizar a los algoritmos dependiendo de la naturaleza de cada problema. Esta caracterizaci贸n es fundamental porque refleja los diferentes tipos de informaci贸n y feedback disponibles en problemas del mundo real, lo que determina qu茅 algoritmos y enfoques son aplicables en cada situaci贸n. A grandes rasgos, la distinci贸n se basa en la naturaleza de los datos disponibles y el tipo de aprendizaje requerido: cuando se tienen suficientes etiquetados usamos aprendizaje supervisado para tareas predictivas; cuando se tienen etiquetas pero se quiere descubrir patrones ocultos se usan t茅cnicas no supervisadas; cuando las etiquetas son escasas pero se cuenta con muchos datos sin etiquetar, el enfoque semi-supervisado permite aprovechar ambos tipos de informaci贸n; y cuando se necesita que un sistema aprenda a trav茅s de la experiencia e interacci贸n con su entorno, el aprendizaje por refuerzo es la opci贸n natural. Esta taxonom铆a no s贸lo organiza conceptualmente al machine learning como disciplina cient铆fica, sino que gu铆a la forma en c贸mo seleccionar las metodolog铆as m谩s apropiadas seg煤n las caracter铆sticas espec铆ficas de los datos y objetivos, intentando optimizar los recursos y resultados de los proyectos de machine learning.

**Aprendizaje Supervisado**: Este tipo de aprendizaje utiliza conjuntos de datos etiquetados donde cada observaci贸n tiene una etiqueta (ya sea num茅rica o categ贸rica) conocida y correcta. El algoritmo aprende a mapear las entradas a las salidas correctas mediante la observaci贸n de estos pares entrada-salida durante el entrenamiento. Su objetivo principal es construir un modelo que pueda predecir con precisi贸n las etiquetas o valores de nuevos datos no vistos anteriormente. Se divide en dos categor铆as: clasificaci贸n (predecir categor铆as discretas como spam/no spam) y regresi贸n (predecir valores continuos como precios de casas).

Claro, aqu铆 tienes los **principales algoritmos de aprendizaje supervisado**, agrupados por tipo, con una breve descripci贸n:

---

###  **1. Regresi贸n**

Modelan una **variable continua** (output num茅rico):

1. **Regresi贸n Lineal (Linear Regression)**: Ajusta una l铆nea recta a los datos. Su f贸rmula: $y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \varepsilon$

2. **Regresi贸n Log铆stica (Logistic Regression)**: Se utiliza para  variables binarias categ贸ricas.Usa la funci贸n log铆stica para modelar la probabilidad:
     $P(y=1|x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p)}}$

3. **Regresi贸n Ridge y Lasso (Regularizada)**: Introducen penalizaci贸n para evitar overfitting:
     * Ridge: penaliza con $\lambda \sum \beta_j^2$
     * Lasso: penaliza con $\lambda \sum |\beta_j|$

4. **K-Nearest Neighbors (KNN)**: Clasifica seg煤n los "k" vecinos m谩s cercanos.
5. **rboles de Decisi贸n (Decision Trees)**: Divide el espacio de variables explicativas en regiones homog茅neas.
6. **Random Forest**: Es un conjunto de 谩rboles de decisi贸n (ensamble) que educe la varianza y mejora la generalizaci贸n.
7. **Support Vector Machines (SVM)**: Intentan encuontrar el hiperplano 贸ptimo que separa clases. Es 煤til en espacios de alta dimensi贸n.
8. **Naive Bayes**: Se basado en probabilidades condicionales y el teorema de Bayes y supone independencia entre variables predictoras.
9. **Redes Neuronales Artificiales (ANN)**: Se componen de capas de neuronas artificiales, son muy flexibles, pero pueden sobreajustarse si no se regularizan.
10. **Gradient Boosting Machines (GBM)**: Construye modelos secuenciales que corrigen errores anteriores. Ejemplos populares: XGBoost, LightGBM, CatBoost.
11. **AdaBoost**: Pone m谩s peso a los errores anteriores en cada iteraci贸n.
12. **Stacking (Apilamiento)**:Mezcla modelos base y usa otro modelo para combinarlos.


**Aprendizaje no supervisado**: En este enfoque, el algoritmo trabaja con datos que no tienen etiquetas o respuestas conocidas, en el que se busca descubrir patrones ocultos y estructuras en los datos por s铆 mismo. El objetivo es encontrar representaciones 煤tiles de los datos, agrupar elementos similares, reducir dimensionalidad o detectar anomal铆as sin gu铆a externa. Las t茅cnicas m谩s comunes incluyen clustering (agrupamiento), reducci贸n de dimensionalidad, y detecci贸n de patrones, siendo especialmente valioso para explorar datos desconocidos y encontrar insights que no son evidentes a simple vista.

**Aprendizaje Semi-supervisado**: Esta metodolog铆a combina elementos del aprendizaje supervisado y no supervisado, utiliza una peque帽a cantidad de datos etiquetados junto con una gran cantidad de datos sin etiquetar. El objetivo es aprovechar la gran cantidad informaci贸n no etiquetada para mejorar el rendimiento del modelo m谩s all谩 de lo que ser铆a posible usando s贸lo los limitados datos etiquetados. Es particularmente 煤til cuando obtener etiquetas es costoso o requiere mucho tiempo, como en reconocimiento de im谩genes m茅dicas o procesamiento de lenguaje natural, donde se puede usar texto abundante sin anotar para mejorar modelos entrenados con pocos ejemplos etiquetados.

**Aprendizaje por refuerzo**: En este paradigma, un agente aprende a tomar decisiones 贸ptimas en un entorno mediante la interacci贸n directa, recibiendo recompensas o castigos basados en sus acciones. El objetivo es que el agente desarrolle una pol铆tica de comportamiento que maximice la recompensa acumulada a largo plazo, aprendiendo qu茅 acciones tomar en diferentes situaciones a trav茅s de prueba y error. No requiere necesariamente datos etiquetados, sino que aprende de las consecuencias de sus propias acciones. Es fundamental en aplicaciones como juegos, rob贸tica, sistemas de recomendaci贸n y control aut贸nomo, donde el agente debe aprender estrategias 贸ptimas para alcanzar objetivos espec铆ficos.

En PySpark (la API de Python para Apache Spark), el m贸dulo `pyspark.ml` proporciona varios **algoritmos de aprendizaje supervisado**, tanto para **clasificaci贸n** como para **regresi贸n**.

### Clasificaci贸n (`pyspark.ml.classification`)

1. **Logistic Regression**
   `LogisticRegression()`
   Ideal para clasificaci贸n binaria y multiclase (mediante softmax).

2. **Decision Tree Classifier**
   `DecisionTreeClassifier()`
   rbol de decisi贸n para clasificaci贸n.

3. **Random Forest Classifier**
   `RandomForestClassifier()`
   Ensamble de 谩rboles; mejora precisi贸n y reduce overfitting.

4. **Gradient-Boosted Tree (GBT) Classifier**
   `GBTClassifier()`
   Variante de boosting sobre 谩rboles; muy potente para clasificaci贸n binaria.

5. **Naive Bayes**
   `NaiveBayes()`
   Modelo probabil铆stico, 煤til con texto y datos categ贸ricos.

6. **Multilayer Perceptron Classifier**
   `MultilayerPerceptronClassifier()`
   Red neuronal multicapa; 煤til para clasificaci贸n no lineal.

7. **Linear Support Vector Machine (SVM)**
   `LinearSVC()`
   SVM para clasificaci贸n binaria (con margen suave).

### Regresi贸n (`pyspark.ml.regression`)

1. **Linear Regression**
   `LinearRegression()`
   Modelo lineal cl谩sico.

2. **Generalized Linear Regression (GLR)**
   `GeneralizedLinearRegression()`
   Ampliaci贸n del modelo lineal para diferentes distribuciones (Poisson, Gamma, etc.).

3. **Decision Tree Regressor**
   `DecisionTreeRegressor()`
   rbol de decisi贸n para regresi贸n.

4. **Random Forest Regressor**
   `RandomForestRegressor()`
   Ensamble de 谩rboles para regresi贸n.

5. **Gradient-Boosted Tree Regressor**
   `GBTRegressor()`
   Boosting sobre 谩rboles para regresi贸n.

6. **Isotonic Regression**
   `IsotonicRegression()`
   Regresi贸n no param茅trica, 煤til cuando se asume una relaci贸n mon贸tonamente creciente o decreciente.


### Otros Componentes Importantes

* **Pipelines y Transformers** para preprocesamiento: `VectorAssembler`, `StringIndexer`, `StandardScaler`, etc.
* **Evaluadores**: `BinaryClassificationEvaluator`, `MulticlassClassificationEvaluator`, `RegressionEvaluator`.

En **PySpark**, los algoritmos de **aprendizaje no supervisado** est谩n disponibles principalmente en el m贸dulo `pyspark.ml.clustering` (para *clustering*) y `pyspark.ml.fpm` (para *pattern mining*). Los principales algoritmos no supervisados que se puede usar son:

1. **K-Means**
   `KMeans()`
   Clustering basado en centroides. Requiere especificar el n煤mero de clusters `k`.

2. **Bisecting K-Means**
   `BisectingKMeans()`
   Variante jer谩rquica de K-Means, 煤til para estructuras de cluster en 谩rbol.

3. **Gaussian Mixture Model (GMM)**
   `GaussianMixture()`
   Modelo probabil铆stico que supone que los datos provienen de una mezcla de distribuciones normales. Devuelve probabilidades de pertenencia.

4. **Latent Dirichlet Allocation (LDA)**
   `LDA()`
   Modelo probabil铆stico para an谩lisis de t贸picos en texto. Agrupa documentos por temas latentes.

5. **Power Iteration Clustering (PIC)**
   `PowerIterationClustering()`
   Algoritmo de clustering espectral escalable, basado en el an谩lisis del grafo de similitud entre elementos.

6. **Frequent Pattern Growth (FP-Growth)**
   `FPGrowth()`
   Algoritmo eficiente para encontrar conjuntos frecuentes y generar reglas de asociaci贸n. til en an谩lisis de cestas de mercado (*market basket analysis*).


Aunque no est谩n en un m贸dulo espec铆fico de "unsupervised", PySpark ofrece soporte para algunos algoritmos de reducci贸n de dimensiones, 煤tiles para tareas no supervisadas:

7. **Principal Component Analysis (PCA)**
   `PCA()`
   Reducci贸n de dimensionalidad lineal, mantiene la varianza m谩xima.

8. **Truncated Singular Value Decomposition (SVD)**
   `TruncatedSVD()`
   Reducci贸n lineal de dimensiones (en Spark solo disponible para matrices dispersas como `RowMatrix` en `pyspark.mllib.linalg.distributed`).

# Partes: 2. Selecci贸n de los datos, 3. Preparaci贸n de los datos, 4. Preparaci贸n del conjunto de entrenamiento y prueba y 5. Construcci贸n de modelos de aprendizaje supervisado y no supervisado

El desarrollo de estas secciones se encuenta en el el jupyter notebook que se encuentra en este repositorio en la liga:

[Jupyter Notebook](./ProyectoEntrega3ML_A01795167.ipynb)

## Referencias

+ https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html
